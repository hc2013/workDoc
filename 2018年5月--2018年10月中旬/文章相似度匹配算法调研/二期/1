二期首先要解决的问题就是句子模糊的问题，

	现在已经明确，对文章句子中进行修改，将会无法发现相似度，考虑使用lucene fuzzy query



==================================================================

现在的问题是，直接使用fuzzyQuery，edit distance只能是2,显然不符合句子模糊的需求
	可是目前又找不到办法将其扩大超过2，这涉及到核心逻辑，代价和风险都比较大
		涉及到FST的seek问题


转而想用TermVectors替代Index，这样就绕开了FST的状态机的问题

	那么现在的问题是，TermVector的量级很大，如果存储所有的分句结果，那么线性遍历再匹配的时间开销将会非常夸张


	目前的想法是，重写一款分词器，返回的序列是关键句子序列，那么无论是Index还是TermVector，都是只存关键句子，比如50个关键句子

	等到target来的时候，直接线性匹配所有的TermVector

	那么数量级应该在50×50×n这个级别


现在得到数据
	在50×50这个环境下
	3453ms,大概可以处理338篇文章
		目前不清楚句子的关键词提取所占的时间是多少

	需要测试，打球回来测试

============================================================================================================================

1.Lucene原声MoreLikeThis的局限
	








思路，修改分词器，分词的时候，直接返回××××关键句的分词结果（列表）×××××××
					在匹配的时候，其思路是，匹配誓词结果    


